# Archi - Copy to .env and fill in

# Context window size (tokens)
# Qwen3-VL supports 256K, but larger = more VRAM
# 32K = good balance for 8GB VRAM (default)
# 16K = conservative for 6GB VRAM
# 8K = safe for 4GB VRAM
# ARCHI_CONTEXT_SIZE=32768

# Local model (Gate B/C) - path to Qwen3VL GGUF (vision + reasoning)
# Place Qwen3VL-8B-Instruct-Q4_K_M.gguf + mmproj-Qwen3VL-8B-Instruct-F16.gguf in models/
# Requires JamePeng llama-cpp-python 0.3.24 fork for vision. See RUN.md.
# LOCAL_MODEL_PATH=C:/Archi/models/Qwen3VL-8B-Instruct-Q4_K_M.gguf
LOCAL_MODEL_PATH=

# Optional: CUDA toolkit root (for GPU build of llama-cpp-python)
# CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.1

# Optional: base path for logs, data, workspace (default: repo root)
# ARCHI_ROOT=C:/Archi

# Frontier APIs (Gate B+)
# x.ai Grok: https://api.x.ai – get key at x.ai
GROK_API_KEY=gsk_replace_with_your_key
# ANTHROPIC_API_KEY=
# GOOGLE_API_KEY=

# Gate C – Computer Use (optional)
# START_BUTTON_X=843           # px or fraction (e.g. 0.33)
# SKIP_GROK=1                  # Disable Grok vision fallback
# DEBUG_CLICK=1                # Save annotated screenshot

# Discord bot (Gate G) - https://discord.com/developers/applications
# DISCORD_BOT_TOKEN=your_bot_token

# Limits
# DAILY_BUDGET_USD=5.00
# LOG_LEVEL=INFO

# Prefer local model (reduce Grok usage)
# ARCHI_PREFER_LOCAL_STRICT=1     # Never escalate to Grok when prefer_local=True (accept local even if low confidence)
# ARCHI_CONVERSATIONAL_WORD_LIMIT=25   # Max user words for conversational threshold (default 25)
