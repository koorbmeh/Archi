# Archi - Copy to .env and fill in

# Local model (Gate B/C) - path to Qwen3VL GGUF (vision + reasoning)
# Place Qwen3VL-8B-Instruct-Q4_K_M.gguf + mmproj-Qwen3VL-8B-Instruct-F16.gguf in models/
# Requires JamePeng llama-cpp-python 0.3.24 fork for vision. See RUN.md.
# LOCAL_MODEL_PATH=C:/Archi/models/Qwen3VL-8B-Instruct-Q4_K_M.gguf
LOCAL_MODEL_PATH=

# Optional: CUDA toolkit root (for GPU build of llama-cpp-python)
# CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.1

# Optional: base path for logs, data, workspace (default: repo root)
# ARCHI_ROOT=C:/Archi

# Frontier APIs (Gate B+)
# x.ai Grok: https://api.x.ai – get key at x.ai
GROK_API_KEY=gsk_replace_with_your_key
# ANTHROPIC_API_KEY=
# GOOGLE_API_KEY=

# Gate C – Computer Use (optional)
# START_BUTTON_X=843           # px or fraction (e.g. 0.33)
# SKIP_GROK=1                  # Disable Grok vision fallback
# DEBUG_CLICK=1                # Save annotated screenshot

# Discord bot (Gate G) - https://discord.com/developers/applications
# DISCORD_BOT_TOKEN=your_bot_token

# Limits
# DAILY_BUDGET_USD=5.00
# LOG_LEVEL=INFO
